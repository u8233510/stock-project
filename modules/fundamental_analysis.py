from datetime import datetime, timedelta, timezone
import re
from typing import Any

import requests
import streamlit as st
from ddgs import DDGS

import database
from modules.llm_model_selector import get_llm_model


def _search_news(ddgs: DDGS, query: str, timelimit: str) -> list[dict]:
    """ç›¸å®¹ä¸åŒ ddgs ç‰ˆæœ¬çš„ news åƒæ•¸å‘½åã€‚"""
    common_kwargs = {
        "region": "wt-wt",
        "safesearch": "off",
        "timelimit": timelimit,
        "max_results": 10,
    }

    attempts = [
        lambda: ddgs.news(query=query, **common_kwargs),
        lambda: ddgs.news(query, **common_kwargs),
        lambda: ddgs.news(keywords=query, **common_kwargs),
    ]

    last_exc = None
    for attempt in attempts:
        try:
            return list(attempt())
        except Exception as exc:
            last_exc = exc
            continue

    if last_exc is not None:
        raise last_exc
    return []


def _is_relevant_news(item: dict, sid: str, sname: str) -> bool:
    """ä»¥è‚¡ç¥¨ä»£ç¢¼/åç¨±åšåŸºç¤ç›¸é—œæ€§éæ¿¾ï¼Œé™ä½ç„¡é—œæ–°èã€‚"""
    text = " ".join(
        [
            str(item.get("title", "")),
            str(item.get("body", "")),
            str(item.get("snippet", "")),
            str(item.get("url", "")),
            str(item.get("href", "")),
            str(item.get("link", "")),
        ]
    ).lower()

    sid_txt = str(sid).strip().lower()
    sname_txt = str(sname).strip().lower()

    sid_hit = sid_txt and sid_txt in text
    name_hit = sname_txt and sname_txt in text
    return bool(sid_hit or name_hit)


def _parse_relative_date(text: str):
    now = datetime.now(timezone.utc)
    raw = str(text).strip().lower()

    m = re.search(r"(\d+)\s*(minute|minutes|min|mins)\s*ago", raw)
    if m:
        return now - timedelta(minutes=int(m.group(1)))

    m = re.search(r"(\d+)\s*(hour|hours|hr|hrs)\s*ago", raw)
    if m:
        return now - timedelta(hours=int(m.group(1)))

    m = re.search(r"(\d+)\s*(day|days)\s*ago", raw)
    if m:
        return now - timedelta(days=int(m.group(1)))

    m = re.search(r"(\d+)\s*(week|weeks)\s*ago", raw)
    if m:
        return now - timedelta(weeks=int(m.group(1)))

    m = re.search(r"(\d+)\s*åˆ†é˜å‰", raw)
    if m:
        return now - timedelta(minutes=int(m.group(1)))

    m = re.search(r"(\d+)\s*å°æ™‚å‰", raw)
    if m:
        return now - timedelta(hours=int(m.group(1)))

    m = re.search(r"(\d+)\s*å¤©å‰", raw)
    if m:
        return now - timedelta(days=int(m.group(1)))

    m = re.search(r"(\d+)\s*é€±å‰", raw)
    if m:
        return now - timedelta(weeks=int(m.group(1)))

    return None


def _parse_news_date(date_str: str):
    if not date_str:
        return datetime.min.replace(tzinfo=timezone.utc)

    txt = str(date_str).strip()
    normalized = txt.replace("Z", "+00:00")
    try:
        dt = datetime.fromisoformat(normalized)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        pass

    fmts = [
        "%Y-%m-%d %H:%M:%S",
        "%Y-%m-%d %H:%M",
        "%Y-%m-%d",
        "%Y/%m/%d %H:%M:%S",
        "%Y/%m/%d %H:%M",
        "%Y/%m/%d",
        "%b %d, %Y",
    ]
    for fmt in fmts:
        try:
            return datetime.strptime(txt, fmt).replace(tzinfo=timezone.utc)
        except Exception:
            continue

    rel = _parse_relative_date(txt)
    if rel is not None:
        return rel

    return datetime.min.replace(tzinfo=timezone.utc)


def _news_sort_key(item: dict) -> float:
    """å›å‚³å¯æ’åºæ•¸å€¼ï¼Œé¿å…ä¸åŒä¾†æºæ—¥æœŸç•°å¸¸é€ æˆæ’åºä¾‹å¤–ã€‚"""
    try:
        dt = _parse_news_date(str(item.get("date", "")))
        if isinstance(dt, datetime):
            return dt.timestamp()
    except Exception:
        pass
    return 0.0


def _build_queries(sid: str, sname: str) -> list[str]:
    return [
        f"{sname} {sid} å°è‚¡ æ–°è",
        f"{sname} {sid} è‚¡ç¥¨ æ–°è",
        f"{sname} è‚¡ç¥¨ æ–°è",
        f"{sid} è‚¡ç¥¨ æ–°è",
    ]


def _fetch_dgs_news(sid: str, sname: str, timelimit: str = "y") -> list[dict]:
    queries = _build_queries(sid, sname)
    news_list: list[dict[str, Any]] = []

    with DDGS() as ddgs:
        best_fallback = []
        for query in queries:
            fetched = _search_news(ddgs, query, timelimit)
            if fetched and not best_fallback:
                best_fallback = fetched

            relevant = [n for n in fetched if _is_relevant_news(n, sid, sname)]
            if relevant:
                news_list = relevant
                break

        if not news_list:
            news_list = best_fallback

    return sorted(news_list, key=_news_sort_key, reverse=True)[:10]


def _fetch_serper_news(sid: str, sname: str, api_key: str) -> list[dict]:
    queries = _build_queries(sid, sname)
    headers = {"X-API-KEY": api_key, "Content-Type": "application/json"}

    best_fallback: list[dict[str, Any]] = []
    for query in queries:
        payload = {"q": query, "num": 10, "gl": "tw", "hl": "zh-tw", "tbs": "qdr:y"}
        resp = requests.post("https://google.serper.dev/news", headers=headers, json=payload, timeout=20)
        resp.raise_for_status()
        data = resp.json() if resp.content else {}
        fetched = data.get("news", []) if isinstance(data, dict) else []
        if fetched and not best_fallback:
            best_fallback = fetched

        relevant = [n for n in fetched if _is_relevant_news(n, sid, sname)]
        if relevant:
            best_fallback = relevant
            break

    normalized = [
        {
            "title": item.get("title", ""),
            "body": item.get("snippet", "") or item.get("body", ""),
            "url": item.get("link", "") or item.get("url", ""),
            "source": item.get("source", "SERPER"),
            "date": item.get("date", ""),
        }
        for item in best_fallback
    ]

    return sorted(normalized, key=_news_sort_key, reverse=True)[:10]


def _render_news_list(news_list: list[dict], source_label: str):
    if not news_list:
        st.warning("ç›®å‰æ‰¾ä¸åˆ°ç›¸é—œæ–°èï¼ˆå·²å˜—è©¦å¤šçµ„é—œéµå­—èˆ‡è¿‘ä¸€å¹´ç¯„åœï¼‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚")
        return

    for news in news_list[:10]:
        with st.container():
            col1, col2 = st.columns([1, 4])

            with col1:
                source = news.get("source", source_label)
                date_str = news.get("date", "")
                if date_str:
                    dt = _parse_news_date(date_str)
                    if dt != datetime.min.replace(tzinfo=timezone.utc):
                        st.caption(f"ğŸ“… {dt.astimezone(timezone.utc).strftime('%m/%d %H:%M')}")
                    else:
                        st.caption(date_str)
                st.info(f"ğŸ“ {source}")

            with col2:
                title = news.get("title", "(ç„¡æ¨™é¡Œ)")
                url = news.get("url") or news.get("href", "") or news.get("link", "")
                snippet = news.get("body", "é»æ“Šæ¨™é¡ŒæŸ¥çœ‹å®Œæ•´å…§å®¹...")
                if url:
                    st.markdown(f"#### [{title}]({url})")
                else:
                    st.markdown(f"#### {title}")
                st.write(f"{snippet[:150]}...")

            st.divider()


def _summarize_news(cfg: dict, sid: str, sname: str, source_label: str, news_list: list[dict]) -> str:
    if not news_list:
        return "ç›®å‰æ²’æœ‰å¯ä¾›ç¸½çµçš„æ–°èå…§å®¹ã€‚"

    llm_cfg = cfg.get("llm", {}) if isinstance(cfg, dict) else {}
    api_key = llm_cfg.get("api_key", "")
    if not api_key:
        return "âš ï¸ å°šæœªè¨­å®š LLM API Keyï¼ˆllm.api_keyï¼‰ï¼Œç›®å‰ç„¡æ³•ç”¢ç”Ÿæ–°èç¸½çµã€‚"

    news_lines = []
    for idx, item in enumerate(news_list[:10], start=1):
        title = str(item.get("title", "(ç„¡æ¨™é¡Œ)")).strip()
        snippet = str(item.get("body", "")).strip()[:220]
        date_str = str(item.get("date", "")).strip()
        source = str(item.get("source", source_label)).strip()
        url = str(item.get("url") or item.get("href", "") or item.get("link", "")).strip()
        news_lines.append(
            f"{idx}. [{source}] {title}\næ—¥æœŸï¼š{date_str or 'æœªçŸ¥'}\næ‘˜è¦ï¼š{snippet or 'ï¼ˆç„¡æ‘˜è¦ï¼‰'}\né€£çµï¼š{url or 'ï¼ˆç„¡é€£çµï¼‰'}"
        )

    prompt = (
        f"è«‹ä»¥ç¹é«”ä¸­æ–‡ç¸½çµ {sname}ï¼ˆ{sid}ï¼‰çš„ {source_label} æ–°èï¼Œä¸¦è¼¸å‡ºï¼š\n"
        "1) ä¸‰é»é‡é»\n"
        "2) å°è‚¡åƒ¹å¯èƒ½çš„åå¤š/åç©ºå½±éŸ¿ï¼ˆçŸ­æœŸï¼‰\n"
        "3) éœ€è¦è¿½è¹¤çš„é¢¨éšªäº‹ä»¶\n"
        "å…§å®¹è«‹ç²¾ç°¡ã€é¿å…æœæ’°ï¼Œè‹¥è³‡è¨Šä¸è¶³è«‹æ˜ç¢ºæ¨™ç¤ºã€‚\n\n"
        f"æ–°èè³‡æ–™ï¼š\n{chr(10).join(news_lines)}"
    )

    payload = {
        "model": get_llm_model(cfg, "fundamental"),
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­å°è‚¡ç ”ç©¶åŠ©ç†ï¼Œåƒ…èƒ½æ ¹æ“šçµ¦å®šæ–°èé€²è¡Œæ•´ç†ï¼Œä¸å¯æé€ ã€‚"},
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.2,
        "max_tokens": 1200,
    }
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    resp = requests.post("https://integrate.api.nvidia.com/v1/chat/completions", headers=headers, json=payload, timeout=120)
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"]


def _render_summary_button(cfg: dict, sid: str, sname: str, source_label: str, news_list: list[dict], key: str):
    if st.button(f"ğŸ§  ç¸½çµ {source_label} æ–°è", use_container_width=True, key=key):
        with st.spinner(f"æ­£åœ¨æ•´ç† {source_label} æ–°èé‡é»..."):
            try:
                summary = _summarize_news(cfg, sid, sname, source_label, news_list)
                st.markdown(summary)
            except Exception as e:
                st.error(f"{source_label} æ–°èç¸½çµå¤±æ•—ï¼š{str(e)}")


def render_stock_news(sid: str, sname: str, cfg: dict | None = None, serper_api_key: str | None = None):
    """é¡¯ç¤º DGS èˆ‡ SERPER å…©ç¨®ä¾†æºæ–°èï¼ˆæœ€æ–°åˆ°æœ€èˆŠï¼Œæœ€å¤š 10 ç­†ï¼‰ã€‚"""
    st.subheader(f"ğŸŒ {sname} ({sid}) æœ€æ–°ç›¸é—œæ–°è")

    cfg = cfg or database.load_config()
    tab_dgs, tab_serper = st.tabs(["DGS", "SERPER"])

    with tab_dgs:
        try:
            with st.spinner("DGS æ­£åœ¨æœå°‹æœ€æ–°å‹•æ…‹..."):
                dgs_news = _fetch_dgs_news(sid, sname, timelimit="y")
            _render_news_list(dgs_news, "DGS")
            _render_summary_button(cfg, sid, sname, "DGS", dgs_news, key=f"sum_dgs_{sid}")
        except Exception as e:
            st.error(f"DGS æœå°‹æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")

    with tab_serper:
        if not serper_api_key:
            st.warning("æœªè¨­å®š SERPER API Keyï¼ˆsearch.serper_api_keyï¼‰ï¼Œæ­¤åˆ†é ç„¡æ³•æŸ¥è©¢ã€‚")
            return
        try:
            with st.spinner("SERPER æ­£åœ¨æœå°‹æœ€æ–°å‹•æ…‹..."):
                serper_news = _fetch_serper_news(sid, sname, serper_api_key)
            _render_news_list(serper_news, "SERPER")
            _render_summary_button(cfg, sid, sname, "SERPER", serper_news, key=f"sum_serper_{sid}")
        except Exception as e:
            st.error(f"SERPER æœå°‹æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")


def show_fundamental_analysis():
    """ä¿æŒèˆ‡ app.py ç›¸å®¹çš„å…¥å£å‡½æ•¸ã€‚"""
    st.markdown("### ğŸ’ åŸºæœ¬é¢åˆ†æï¼ˆæ–°èï¼‰")

    cfg = database.load_config()
    universe = cfg.get("universe", [])
    if not universe:
        st.error("universe æœªè¨­å®šï¼Œè«‹å…ˆåœ¨è¨­å®šæª”é…ç½®æ¨™çš„ã€‚")
        return

    stock_options = {f"{s['stock_id']} {s['name']}": (s["stock_id"], s["name"]) for s in universe}
    selected_label = st.selectbox("é¸æ“‡è‚¡ç¥¨", list(stock_options.keys()))
    sid, sname = stock_options[selected_label]
    serper_api_key = cfg.get("search", {}).get("serper_api_key", "")

    if st.button("ğŸ” æœå°‹æœ€æ–°æ–°è", use_container_width=True):
        render_stock_news(sid, sname, cfg=cfg, serper_api_key=serper_api_key)


if __name__ == "__main__":
    st.set_page_config(page_title="è‚¡ç¥¨æ–°èæœå°‹", layout="wide")
    render_stock_news("2233", "å®‡éš†")
