import streamlit as st
import pandas as pd
import database
import requests
from duckduckgo_search import DDGS



def _normalize_secret(value):
    """å»é™¤å¸¸è¦‹è²¼ä¸Šæ±¡æŸ“ï¼ˆç©ºç™½/æ›è¡Œ/BOMï¼‰ã€‚"""
    if value is None:
        return ""
    return str(value).replace("ï»¿", "").strip()


def _google_error_hint(err_msg):
    """å°‡ Google API å¸¸è¦‹éŒ¯èª¤è½‰æˆå¯æ“ä½œå»ºè­°ã€‚"""
    msg = (err_msg or "").lower()
    if "api key not valid" in msg or "invalid" in msg:
        return "è«‹ç¢ºèªï¼š1) API key å±¬æ–¼åŒä¸€å€‹ GCP å°ˆæ¡ˆï¼›2) å·²å•Ÿç”¨ Custom Search JSON APIï¼›3) key æ²’æœ‰è¢« HTTP referrer/IP é™åˆ¶æ“‹ä½æ­¤åŸ·è¡Œç’°å¢ƒã€‚"
    if "referer" in msg or "ip" in msg or "not allowed" in msg:
        return "æ­¤é‡‘é‘°é™åˆ¶ä¸ç¬¦ï¼ˆHTTP referrer/IPï¼‰ã€‚è‹¥åœ¨æœ¬æ©Ÿ Python å¾Œç«¯å‘¼å«ï¼Œè«‹ç§»é™¤ referrer é™åˆ¶æˆ–æ”¹ç”¨å…è¨±è©²ä¾†æºçš„é‡‘é‘°ã€‚"
    if "quota" in msg or "rate limit" in msg:
        return "å·²é”é…é¡ä¸Šé™ï¼Œè«‹æª¢æŸ¥ GCP Quotas/è¨ˆè²»è¨­å®šã€‚"
    if "access not configured" in msg or "has not been used" in msg:
        return "å°šæœªå•Ÿç”¨ Custom Search JSON APIï¼Œè«‹åˆ° Google Cloud Console å•Ÿç”¨å¾Œå†è©¦ã€‚"
    return "è«‹æª¢æŸ¥ API key æ˜¯å¦æ­£ç¢ºã€Custom Search JSON API æ˜¯å¦å•Ÿç”¨ã€ä»¥åŠ key é™åˆ¶æ˜¯å¦å…è¨±ç›®å‰åŸ·è¡Œç’°å¢ƒã€‚"

PUTER_JS_SNIPPET = """<script src="https://js.puter.com/v2/"></script>
<script>
async function runPuterDemo() {
  try {
    const response = await puter.ai.chat(
      "é‡å­é‹ç®—çš„æœ€æ–°é€²å±•æ˜¯ä»€éº¼ï¼Ÿ",
      { model: "perplexity/sonar" }
    );
    console.log(response);
  } catch (err) {
    console.error("Puter å‘¼å«å¤±æ•—:", err);
  }
}
runPuterDemo();
</script>
"""


def _google_search(query, cfg, max_results=5):
    """é€é Google Custom Search å–å¾—æœå°‹æ‘˜è¦ã€‚"""
    search_cfg = cfg.get("search", {})
    api_key = _normalize_secret(search_cfg.get("google_api_key"))
    cse_id = _normalize_secret(search_cfg.get("google_cse_id"))
    if not api_key or not cse_id:
        return [], "Google æœªè¨­å®š google_api_key æˆ– google_cse_idã€‚"

    try:
        resp = requests.get(
            "https://www.googleapis.com/customsearch/v1",
            params={
                "key": api_key,
                "cx": cse_id,
                "q": query,
                "num": min(max_results, 10),
                "hl": "zh-TW"
            },
            timeout=20,
        )
        data = resp.json()
        if resp.status_code >= 400:
            err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
            hint = _google_error_hint(err_msg)
            return [], f"Google æœå°‹å¤±æ•—ï¼š{err_msg}ï½œå»ºè­°ï¼š{hint}"

        items = data.get("items", []) if isinstance(data, dict) else []
        records = [
            {
                "source": "Google",
                "title": i.get("title", ""),
                "snippet": i.get("snippet", ""),
                "url": i.get("link", ""),
            }
            for i in items
        ]
        if not records:
            return [], "Google å·²é€£ç·šï¼Œä½†æ­¤æŸ¥è©¢ç„¡çµæœã€‚"
        return records, None
    except Exception as exc:
        return [], f"Google æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _perplexity_search(query, cfg):
    """é€é Perplexity API å–å¾—å¤–éƒ¨è³‡è¨Šæ‘˜è¦ã€‚"""
    search_cfg = cfg.get("search", {})
    api_key = _normalize_secret(search_cfg.get("perplexity_api_key"))
    model = search_cfg.get("perplexity_model", "sonar")
    if not api_key:
        return [], "Perplexity æœªè¨­å®š perplexity_api_keyã€‚"

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ç ”ç©¶åŠ©ç†ï¼Œè«‹æ ¹æ“šæœ€æ–°ç¶²è·¯å…¬é–‹è³‡è¨Šæ•´ç†é‡é»ï¼Œä¸¦é™„ä¸Šä¾†æºç¶²å€ã€‚",
            },
            {
                "role": "user",
                "content": f"è«‹é‡å°ä»¥ä¸‹ä¸»é¡Œæ•´ç† 5 é»é‡é»ï¼Œæ ¼å¼ç‚ºä¸€è¡Œä¸€é»ï¼Œä¸”æ¯é»é™„ä¾†æºç¶²å€ï¼š{query}",
            },
        ],
        "temperature": 0.0,
    }

    try:
        resp = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=payload,
            timeout=30,
        )
        data = resp.json()
        if resp.status_code >= 400:
            err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
            return [], f"Perplexity æœå°‹å¤±æ•—ï¼š{err_msg}"

        content = data.get("choices", [{}])[0].get("message", {}).get("content", "") if isinstance(data, dict) else ""
        if not content:
            return [], "Perplexity å·²é€£ç·šï¼Œä½†ç„¡å›å‚³å…§å®¹ã€‚"
        return [{"source": "Perplexity", "title": "æ‘˜è¦", "snippet": content, "url": ""}], None
    except Exception as exc:
        return [], f"Perplexity æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _ddg_search(query, max_results=5, source="DuckDuckGo"):
    try:
        with DDGS() as ddgs:
            results = []
            for r in ddgs.text(query, max_results=max_results):
                results.append(
                    {
                        "source": source,
                        "title": r.get("title", ""),
                        "snippet": r.get("body", ""),
                        "url": r.get("href", ""),
                    }
                )
            if not results:
                return [], f"{source} æŸ¥è©¢ç„¡çµæœã€‚"
            return results, None
    except Exception as exc:
        return [], f"{source} æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _build_external_context(stock_name, sid, cfg):
    """è’é›†å¤–éƒ¨è³‡è¨Šï¼ˆPerplexity / Google / DDG / ç¤¾ç¾¤ç¶²ç«™æœå°‹ï¼‰ã€‚"""
    base_query = f"{stock_name} {sid} æ ¸å¿ƒç”¢å“ ç”¢æ¥­åœ°ä½ æœ€æ–°æ–°è"

    records = []
    warnings = []

    per_records, per_warn = _perplexity_search(base_query, cfg)
    records.extend(per_records)
    if per_warn:
        warnings.append(per_warn)

    google_records, google_warn = _google_search(base_query, cfg, max_results=5)
    records.extend(google_records)
    if google_warn:
        warnings.append(google_warn)

    ddg_records, ddg_warn = _ddg_search(base_query, max_results=5, source="DuckDuckGo")
    records.extend(ddg_records)
    if ddg_warn:
        warnings.append(ddg_warn)

    # ç¤¾ç¾¤/è¼¿æƒ…ï¼ˆä»¥å…¬é–‹å¯ç´¢å¼•é é¢ç‚ºä¸»ï¼Œéç™»å…¥è³‡æ–™ï¼‰
    social_queries = [
        (f"site:x.com OR site:twitter.com {stock_name} {sid}", "X/Twitter"),
        (f"site:facebook.com {stock_name} {sid}", "Facebook"),
        (f"site:instagram.com {stock_name} {sid}", "Instagram"),
    ]
    for query, source in social_queries:
        social_records, social_warn = _ddg_search(query, max_results=3, source=source)
        records.extend(social_records)
        if social_warn:
            warnings.append(social_warn)

    if not records:
        warnings.insert(0, "ç›®å‰æœªå–å¾—å¤–éƒ¨ä¾†æºã€‚è«‹å…ˆæª¢æŸ¥ä¸‹æ–¹å„ä¾†æºè¨ºæ–·è¨Šæ¯ã€‚")
        return "", warnings

    source_counts = {}
    for rec in records:
        src = rec.get("source", "ä¾†æº")
        source_counts[src] = source_counts.get(src, 0) + 1
    summary = "ã€".join([f"{k}:{v}" for k, v in source_counts.items()])
    warnings.insert(0, f"å¤–éƒ¨ä¾†æºæŠ“å–æˆåŠŸï¼ˆ{summary}ï¼‰ã€‚")

    lines = []
    for rec in records[:20]:
        url = rec.get("url", "")
        url_text = f"ï¼ˆ{url}ï¼‰" if url else ""
        lines.append(f"- [{rec.get('source', 'ä¾†æº')}] {rec.get('title', '')}: {rec.get('snippet', '')} {url_text}")
    return "\n".join(lines), warnings


def _build_fundamental_prompt(stock_name, sid, search_ctx, metrics):
    """å»ºç«‹å›ºå®šç« ç¯€æ ¼å¼çš„åŸºæœ¬é¢åˆ†æ Promptã€‚"""
    return f"""
è«‹ä½ æ‰®æ¼”å°è‚¡è³‡æ·±åŸºæœ¬é¢åˆ†æå¸«ï¼Œé‡å° {stock_name}ï¼ˆ{sid}ï¼‰æ’°å¯«å ±å‘Šã€‚

ã€é‡è¦è¦å‰‡ã€‘
1) åš´æ ¼ä½¿ç”¨ä»¥ä¸‹å›ºå®šæ ¼å¼èˆ‡æ¨™é¡Œé †åºï¼Œä¸è¦å¢æ¸›ç« ç¯€ã€‚
2) è‹¥è³‡æ–™ä¸è¶³ï¼Œè«‹æ˜ç¢ºå¯«ã€Œæœªæä¾›ã€æˆ–ã€Œè³‡æ–™ä¸è¶³ã€ï¼Œç¦æ­¢è™›æ§‹ã€‚
3) æ‰€æœ‰æ•¸å€¼ç›¡é‡å¼•ç”¨æˆ‘æä¾›çš„è³‡æ–™ï¼›è‹¥å¼•ç”¨æ–°èï¼Œåƒ…èƒ½ä½¿ç”¨ã€Œæœå°‹äº‹å¯¦æ‘˜è¦ã€ã€‚
4) ä»¥ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚

ã€å›ºå®šè¼¸å‡ºæ ¼å¼ã€‘
## å…¬å¸ç°¡ä»‹
ï¼ˆå…¬å¸å®šä½ã€æ ¸å¿ƒç”¢å“/æœå‹™ã€ä¸»è¦å¸‚å ´ï¼‰

## è²¡å‹™åˆ†æ
ï¼ˆæ•´é«”ç²åˆ©èƒ½åŠ›èˆ‡è¿‘æ³ï¼Œ2-4 å¥ï¼‰

### è²¡å‹™æŒ‡æ¨™
- EPSï¼š
- ROEï¼ˆè‚¡æ±æ¬Šç›Šå ±é…¬ç‡ï¼‰ï¼š
- ROAï¼ˆè³‡ç”¢å ±é…¬ç‡ï¼‰ï¼š
- ç‡Ÿæ”¶æˆé•·ç‡ï¼š
- æ¯›åˆ©ç‡ï¼š

## ç‡Ÿæ”¶åˆ†æ
ï¼ˆç‡Ÿæ”¶è¶¨å‹¢ã€å¯èƒ½é©…å‹•å› å­ï¼‰

## æ¯›åˆ©ç‡åˆ†æ
ï¼ˆæ¯›åˆ©ç‡æ°´æº–èˆ‡å¯èƒ½åŸå› ï¼›è‹¥ç„¡è³‡æ–™è«‹å¯«æœªæä¾›ï¼‰

## ç¾é‡‘æµé‡åˆ†æ
ï¼ˆç¾é‡‘æµé‡ç‹€æ³èˆ‡å“è³ªï¼›è‹¥ç„¡è³‡æ–™è«‹å¯«æœªæä¾›ï¼‰

## æŠ•è³‡è©•åƒ¹
- çŸ­æœŸè©•åƒ¹ï¼š
- ä¸­æœŸè©•åƒ¹ï¼š
- é•·æœŸè©•åƒ¹ï¼š
- ç›®æ¨™åƒ¹æ ¼ï¼š

## é¢¨éšªè©•ä¼°
- å¸‚å ´é¢¨éšªï¼š
- è²¡å‹™é¢¨éšªï¼š
- æ³•è¦/æ”¿ç­–é¢¨éšªï¼š

## çµè«–
ï¼ˆç¸½çµæŠ•è³‡è§€é»èˆ‡é—œéµè¿½è¹¤æŒ‡æ¨™ï¼‰

ã€å¯ç”¨è³‡æ–™ã€‘
- æœå°‹äº‹å¯¦æ‘˜è¦ï¼š{search_ctx if search_ctx else 'æœªæä¾›'}
- æœ€æ–°å­£åº¦ EPSï¼š{metrics.get('latest_eps', 'æœªæä¾›')}
- ä¸Šå­£ EPSï¼š{metrics.get('prev_eps', 'æœªæä¾›')}
- è¿‘ 12 æœˆæœ€æ–°ç‡Ÿæ”¶ï¼ˆå„„å…ƒï¼‰ï¼š{metrics.get('latest_revenue', 'æœªæä¾›')}
- è¿‘ 12 æœˆæœ€èˆŠç‡Ÿæ”¶ï¼ˆå„„å…ƒï¼‰ï¼š{metrics.get('oldest_revenue', 'æœªæä¾›')}
- ä¼°ç®—ç‡Ÿæ”¶æˆé•·ç‡ï¼ˆæœ€æ–° vs æœ€èˆŠï¼‰ï¼š{metrics.get('revenue_growth', 'æœªæä¾›')}
""".strip()

# 1. æ ¸å¿ƒ AI å‘¼å«å·¥å…· (ä¿æŒç©©å®šï¼Œæœªæ›´å‹•)
def _call_nim_fundamental(cfg, prompt):
    llm_cfg = cfg.get("llm", {})
    url = "https://integrate.api.nvidia.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {llm_cfg.get('api_key')}", "Content-Type": "application/json"}
    payload = {
        "model": llm_cfg.get("model"),
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è­‰åˆ¸åˆ†æå¸«ã€‚è«‹å„ªå…ˆåƒè€ƒé€£ç¶²æœå°‹åˆ°çš„äº‹å¯¦ï¼Œçµåˆè²¡å‹™æ•¸æ“šçµ¦å‡ºå…·é«”çš„æŠ•è³‡è©•åƒ¹ï¼Œåš´ç¦è™›æ§‹å…¬å¸æ¥­å‹™ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    return resp.json()["choices"][0]["message"]["content"]

def show_fundamental_analysis():
    st.markdown("### ğŸ’ åŸºæœ¬é¢æ•¸æ“šå…¨è¦½èˆ‡ AI è¨ºæ–·")
    cfg = database.load_config()
    conn = database.get_db_connection(cfg)
    universe = cfg.get("universe", [])
    stock_options = {f"{s['stock_id']} {s['name']}": s['stock_id'] for s in universe}
    
    selected_stock = st.selectbox("è«‹é¸æ“‡åˆ†ææ¨™çš„", list(stock_options.keys()))
    sid = stock_options[selected_stock]
    
    # çµ±ä¸€åŸºæº–æ—¥
    def_s = pd.to_datetime("today") - pd.Timedelta(days=60)
    def_e = pd.to_datetime("today")
    f_range = st.date_input("æ•¸æ“šè§€å¯ŸåŸºæº–æ—¥", value=[def_s, def_e])
    end_d = f_range[1] if len(f_range) == 2 else def_e

    st.divider()

    # --- 1. æ•¸æ“šæŠ“å–èˆ‡æ ¼å¼åŒ– ---
    
    # (1) æ¯æœˆç‡Ÿæ”¶ï¼šé™¤ä»¥ 1000 ä¸¦åŠ ä¸Šåƒåˆ†ä½ ","
    rev_query = f"SELECT date as 'æ—¥æœŸ', revenue FROM stock_month_revenue_monthly WHERE stock_id='{sid}' ORDER BY date DESC LIMIT 12"
    rev_df = pd.read_sql(rev_query, conn)
    if not rev_df.empty:
        # âœ… ä¿®æ­£é—œéµï¼šçµ±ä¸€æ¬„ä½åç¨±ç‚º 'ç‡Ÿæ”¶(å„„)'ï¼Œä¸¦åŠ ä¸Šåƒåˆ†ä½
        rev_df['ç‡Ÿæ”¶(å„„)'] = (rev_df['revenue'] / 100000000).apply(lambda x: f"{x:,.2f}")
        rev_df = rev_df[['æ—¥æœŸ', 'ç‡Ÿæ”¶(å„„)']]
    
    # (2+4) æ¯å­£ç²åˆ©èˆ‡ EPSï¼šå®‰å…¨è½‰ç½®è™•ç†
    profit_raw = pd.read_sql(f"SELECT date, type, value FROM stock_financial_statements WHERE stock_id='{sid}' AND type IN ('EPS', 'Net Profit') ORDER BY date DESC LIMIT 16", conn)
    if not profit_raw.empty:
        profit_df = profit_raw.pivot(index='date', columns='type', values='value').reset_index()
        # å®‰å…¨é‡å‘½å
        rename_map = {'date': 'å­£åº¦', 'EPS': 'EPS', 'Net Profit': 'æ¯å­£ç²åˆ©'}
        profit_df = profit_df.rename(columns={k: v for k, v in rename_map.items() if k in profit_df.columns})
        # åŠ ä¸Šåƒåˆ†ä½ (æ¯å­£ç²åˆ©)
        if 'æ¯å­£ç²åˆ©' in profit_df.columns:
            profit_df['æ¯å­£ç²åˆ©'] = profit_df['æ¯å­£ç²åˆ©'].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")
    else:
        profit_df = pd.DataFrame()

    # (3+5) æœ¬ç›Šæ¯”èˆ‡æ®–åˆ©ç‡
    val_query = f"SELECT date as 'æ—¥æœŸ', PER as 'æœ¬ç›Šæ¯”', dividend_yield as 'æ®–åˆ©ç‡%' FROM stock_per_pbr_daily WHERE stock_id='{sid}' AND date <= '{end_d}' ORDER BY date DESC LIMIT 10"
    valuation_df = pd.read_sql(val_query, conn)

    # (6+7) ç¾é‡‘è‚¡åˆ©èˆ‡è‚¡æ¯åˆ†é…
    div_query = f"SELECT year as 'å¹´ä»½', CashEarningsDistribution as 'ç¾é‡‘è‚¡åˆ©', StockEarningsDistribution as 'è‚¡ç¥¨è‚¡åˆ©' FROM stock_dividend WHERE stock_id='{sid}' ORDER BY year DESC LIMIT 5"
    div_df = pd.read_sql(div_query, conn)

    # --- 2. è¡¨æ ¼åŒ–é¡¯ç¤ºåˆ†é  ---
    tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ ç‡Ÿæ”¶èˆ‡ç²åˆ©è©³æƒ…", "ğŸ’° è‚¡åˆ©èˆ‡ä¼°å€¼çœ‹æ¿", "ğŸ” AI è¯ç¶²è¶¨å‹¢å ±å‘Š"])

    with tab1:
        st.write("#### 1. æ¯æœˆç‡Ÿæ”¶ (å–®ä½ï¼šç™¾è¬å…ƒ)")
        st.dataframe(rev_df, use_container_width=True, hide_index=True)
        
        st.write("#### 2 & 4. æ¯å­£ç²åˆ©èˆ‡ EPS æ­·ç¨‹")
        if not profit_df.empty:
            # ç¢ºä¿åƒ…é¡¯ç¤ºç¾æœ‰æ¬„ä½
            cols_to_show = [c for c in ['å­£åº¦', 'æ¯å­£ç²åˆ©', 'EPS'] if c in profit_df.columns]
            st.dataframe(profit_df[cols_to_show], use_container_width=True, hide_index=True)
        else:
            st.info("å°šç„¡ç²åˆ©æ•¸æ“šã€‚")

    with tab2:
        st.write("#### 3 & 5. æœ¬ç›Šæ¯”èˆ‡æ®–åˆ©ç‡è®Šå‹•")
        st.dataframe(valuation_df, use_container_width=True, hide_index=True)
        
        st.write("#### 6 & 7. æ­·å¹´è‚¡åˆ©åˆ†é… (ç¾é‡‘èˆ‡è‚¡ç¥¨)")
        if not div_df.empty:
            st.table(div_df)
        else:
            st.info("å°šç„¡è‚¡åˆ©æ­·å²æ•¸æ“šã€‚")

    with tab3:
        st.info("ğŸ’¡ å¤–éƒ¨è³‡è¨Šä¾†æºèªªæ˜ï¼šç›®å‰å¾Œç«¯å·²æ•´åˆ Google / Perplexity / DDG / ç¤¾ç¾¤å…¬é–‹é é¢ï¼Œä¸¦é¡¯ç¤ºå„ä¾†æºè¨ºæ–·è¨Šæ¯ã€‚è‹¥ Google å›å ± key ç„¡æ•ˆï¼Œé€šå¸¸æ˜¯ API å•Ÿç”¨æˆ–é‡‘é‘°é™åˆ¶å•é¡Œã€‚")
        with st.expander("Puter.js å… API Key ä½¿ç”¨æ–¹å¼ï¼ˆå‰ç«¯ç¯„ä¾‹ï¼‰", expanded=False):
            st.markdown("å¯ä»¥ç›´æ¥é€™æ¨£å¯«ï¼Œä½†å»ºè­°ç”¨ `async/await + try/catch`ï¼ˆå¦‚ä¸‹ï¼‰è¼ƒå®¹æ˜“é™¤éŒ¯ã€‚")
            st.code(PUTER_JS_SNIPPET, language="html")
            st.markdown("æ”¯æ´æ¨¡å‹ç¤ºä¾‹ï¼š`perplexity/sonar`ã€`perplexity/sonar-pro`ã€`perplexity/sonar-deep-research`ã€`perplexity/sonar-reasoning-pro`ã€‚")

        # âœ… ä¿ç•™è¯ç¶²æœå°‹é‚è¼¯
        if st.button(f"ğŸš€ å•Ÿå‹• {selected_stock} è¯ç¶²äº‹å¯¦åˆ†æ", use_container_width=True):
            with st.spinner("æ­£åœ¨æœå°‹æœ€æ–°ç”¢æ¥­åœ°ä½èˆ‡å¸‚å ´æ–°è..."):
                search_ctx, search_warnings = _build_external_context(selected_stock, sid, cfg)
                if search_warnings:
                    for w in search_warnings:
                        if w.startswith("å¤–éƒ¨ä¾†æºæŠ“å–æˆåŠŸ"):
                            st.success(w)
                        else:
                            st.warning(w)
                
                # ç²å– AI åƒè€ƒæ•¸æ“š
                latest_eps = profit_df['EPS'].iloc[0] if ('EPS' in profit_df.columns and not profit_df.empty) else "æœªæä¾›"
                prev_eps = profit_df['EPS'].iloc[1] if ('EPS' in profit_df.columns and len(profit_df) > 1) else "æœªæä¾›"

                latest_revenue = rev_df['ç‡Ÿæ”¶(å„„)'].iloc[0] if not rev_df.empty else "æœªæä¾›"
                oldest_revenue = rev_df['ç‡Ÿæ”¶(å„„)'].iloc[-1] if not rev_df.empty else "æœªæä¾›"

                revenue_growth = "æœªæä¾›"
                if not rev_df.empty and len(rev_df) > 1:
                    rev_num = pd.to_numeric(rev_df['ç‡Ÿæ”¶(å„„)'].str.replace(',', ''), errors='coerce')
                    latest_rev_num = rev_num.iloc[0]
                    oldest_rev_num = rev_num.iloc[-1]
                    if pd.notnull(latest_rev_num) and pd.notnull(oldest_rev_num) and oldest_rev_num != 0:
                        revenue_growth = f"{((latest_rev_num - oldest_rev_num) / oldest_rev_num) * 100:.2f}%"

                metrics = {
                    "latest_eps": latest_eps,
                    "prev_eps": prev_eps,
                    "latest_revenue": latest_revenue,
                    "oldest_revenue": oldest_revenue,
                    "revenue_growth": revenue_growth
                }

                prompt = _build_fundamental_prompt(selected_stock, sid, search_ctx, metrics)
                st.markdown(_call_nim_fundamental(cfg, prompt))

    conn.close()
