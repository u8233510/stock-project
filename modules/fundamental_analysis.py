import streamlit as st
import pandas as pd
import database
import requests
import math
from duckduckgo_search import DDGS
from urllib.parse import quote_plus
import xml.etree.ElementTree as ET
from modules.llm_model_selector import get_llm_model


TRUSTED_SOURCE_PATTERNS = {
    "A": [
        "mops.twse.com.tw",
        "twse.com.tw",
        "tpex.org.tw",
        "sec.gov",
        "investor",
        "ir.",
    ],
    "B": [
        "reuters.com",
        "bloomberg.com",
        "cnbc.com",
        "wsj.com",
        "moneydj.com",
        "cnyes.com",
        "udn.com",
    ],
}

TW_US_ADR_MAPPING = {
    "2330": "TSM",  # å°ç©é›»
    "2303": "UMC",  # è¯é›»
}



def _normalize_secret(value):
    """å»é™¤å¸¸è¦‹è²¼ä¸Šæ±¡æŸ“ï¼ˆç©ºç™½/æ›è¡Œ/BOMï¼‰ã€‚"""
    if value is None:
        return ""
    return str(value).replace("ï»¿", "").strip()


def _mask_secret(value, keep=4):
    """é®ç½©æ•æ„Ÿè³‡è¨Šï¼Œé¿å…å®Œæ•´é‡‘é‘°å¤–éœ²ã€‚"""
    val = _normalize_secret(value)
    if not val:
        return "(æœªè¨­å®š)"
    if len(val) <= keep:
        return "*" * len(val)
    return f"{'*' * (len(val) - keep)}{val[-keep:]}"


PUTER_JS_SNIPPET = """<script src="https://js.puter.com/v2/"></script>
<script>
async function runPuterDemo() {
  try {
    const response = await puter.ai.chat(
      "é‡å­é‹ç®—çš„æœ€æ–°é€²å±•æ˜¯ä»€éº¼ï¼Ÿ",
      { model: "perplexity/sonar" }
    );
    console.log(response);
  } catch (err) {
    console.error("Puter å‘¼å«å¤±æ•—:", err);
  }
}
runPuterDemo();
</script>
"""


def _external_cache_get(query, max_age_minutes=120):
    cache = st.session_state.setdefault("external_search_cache", {})
    item = cache.get(query)
    if not item:
        return None
    now_ts = pd.Timestamp.utcnow().timestamp()
    if now_ts - item.get("ts", 0) > max_age_minutes * 60:
        return None
    return item.get("records", [])


def _external_cache_set(query, records):
    cache = st.session_state.setdefault("external_search_cache", {})
    cache[query] = {"ts": pd.Timestamp.utcnow().timestamp(), "records": records}


def _google_news_rss_search(query, max_results=4):
    """å…è²»è£œå¼·ä¾†æºï¼šGoogle News RSSï¼ˆä¸éœ€ä»˜è²»ï¼Œä¸ç”¨ Search Consoleï¼‰ã€‚"""
    try:
        rss_url = f"https://news.google.com/rss/search?q={quote_plus(query)}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
        resp = requests.get(rss_url, timeout=20)
        if resp.status_code >= 400:
            return [], f"Google News RSS å¤±æ•—ï¼šHTTP {resp.status_code}"

        root = ET.fromstring(resp.text)
        items = root.findall('.//item')
        records = []
        for item in items[:max_results]:
            link = (item.findtext('link') or '').strip()
            records.append(
                {
                    "source": "GoogleNewsRSS",
                    "title": (item.findtext('title') or '').strip(),
                    "snippet": (item.findtext('description') or '').strip(),
                    "url": link,
                    "tier": _classify_source_tier(link),
                }
            )

        if not records:
            return [], "Google News RSS æŸ¥è©¢ç„¡çµæœã€‚"
        return records, None
    except Exception as exc:
        return [], f"Google News RSS ä¾‹å¤–ï¼š{str(exc)}"


def _wikipedia_summary_search(stock_name, sid):
    """å…è²»è£œå¼·ä¾†æºï¼šWikipedia æ‘˜è¦ï¼ˆå…¬å¸ç°¡ä»‹/ç”¢æ¥­ç·šç´¢ï¼‰ã€‚"""
    candidates = [
        f"{stock_name}",
        f"{stock_name} {sid}",
    ]
    for q in candidates:
        try:
            url = "https://zh.wikipedia.org/api/rest_v1/page/summary/" + quote_plus(q)
            resp = requests.get(url, timeout=20)
            if resp.status_code >= 400:
                continue
            data = resp.json() if resp.headers.get("content-type", "").startswith("application/json") else {}
            title = data.get("title", "") if isinstance(data, dict) else ""
            extract = data.get("extract", "") if isinstance(data, dict) else ""
            page = data.get("content_urls", {}).get("desktop", {}).get("page", "") if isinstance(data, dict) else ""
            if title and extract:
                return [{
                    "source": "Wikipedia",
                    "title": title,
                    "snippet": extract,
                    "url": page,
                    "tier": "B",
                }], None
        except Exception:
            continue
    return [], "Wikipedia æ‘˜è¦ç„¡çµæœã€‚"


def _perplexity_search(query, cfg):
    """é€é Perplexity API å–å¾—å¤–éƒ¨è³‡è¨Šæ‘˜è¦ã€‚"""
    search_cfg = cfg.get("search", {})
    api_key = _normalize_secret(search_cfg.get("perplexity_api_key"))
    model = search_cfg.get("perplexity_model", "sonar")
    if not api_key:
        return [], "Perplexity æœªè¨­å®š perplexity_api_keyã€‚"

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ç ”ç©¶åŠ©ç†ï¼Œè«‹æ ¹æ“šæœ€æ–°ç¶²è·¯å…¬é–‹è³‡è¨Šæ•´ç†é‡é»ï¼Œä¸¦é™„ä¸Šä¾†æºç¶²å€ã€‚",
            },
            {
                "role": "user",
                "content": f"è«‹é‡å°ä»¥ä¸‹ä¸»é¡Œæ•´ç† 5 é»é‡é»ï¼Œæ ¼å¼ç‚ºä¸€è¡Œä¸€é»ï¼Œä¸”æ¯é»é™„ä¾†æºç¶²å€ï¼š{query}",
            },
        ],
        "temperature": 0.0,
    }

    try:
        resp = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers=headers,
            json=payload,
            timeout=30,
        )
        data = resp.json()
        if resp.status_code >= 400:
            err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
            return [], f"Perplexity æœå°‹å¤±æ•—ï¼š{err_msg}"

        content = data.get("choices", [{}])[0].get("message", {}).get("content", "") if isinstance(data, dict) else ""
        if not content:
            return [], "Perplexity å·²é€£ç·šï¼Œä½†ç„¡å›å‚³å…§å®¹ã€‚"
        return [{"source": "Perplexity", "title": "æ‘˜è¦", "snippet": content, "url": ""}], None
    except Exception as exc:
        return [], f"Perplexity æœå°‹ä¾‹å¤–ï¼š{str(exc)}"



def _puter_search(query, cfg):
    """é€é Puter API å–å¾—å¤–éƒ¨è³‡è¨Šæ‘˜è¦ï¼ˆå¯é¸ providerï¼Œé è¨­ä¸å•Ÿç”¨ï¼‰ã€‚"""
    search_cfg = cfg.get("search", {})
    api_key = _normalize_secret(search_cfg.get("puter_api_key"))
    model = search_cfg.get("puter_model", "perplexity/sonar")
    if not api_key:
        return [], "Puter æœªè¨­å®š puter_api_keyã€‚"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ç ”ç©¶åŠ©ç†ï¼Œè«‹æ ¹æ“šå…¬é–‹ç¶²è·¯è³‡è¨Šæ•´ç†é‡é»ä¸¦é™„ä¾†æºç¶²å€ã€‚"},
            {"role": "user", "content": f"è«‹é‡å°ä»¥ä¸‹ä¸»é¡Œæ•´ç† 5 é»é‡é»ï¼Œæ¯é»éœ€é™„ä¾†æºç¶²å€ï¼š{query}"},
        ],
        "temperature": 0.1,
    }

    # æ³¨æ„ï¼šPuter å®˜æ–¹ä»‹é¢å¯èƒ½èª¿æ•´ï¼Œæ•…æ­¤ provider é è¨­ç‚ºå¯é¸ã€‚
    try:
        resp = requests.post(
            "https://api.puter.com/v2/ai/chat/completions",
            headers=headers,
            json=payload,
            timeout=30,
        )
        data = resp.json() if resp.content else {}
        if resp.status_code >= 400:
            err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
            return [], f"Puter æœå°‹å¤±æ•—ï¼š{err_msg}"

        content = data.get("choices", [{}])[0].get("message", {}).get("content", "") if isinstance(data, dict) else ""
        if not content:
            return [], "Puter å·²é€£ç·šï¼Œä½†ç„¡å›å‚³å…§å®¹ã€‚"
        return [{"source": "Puter", "title": "æ‘˜è¦", "snippet": content, "url": "", "tier": "B"}], None
    except Exception as exc:
        return [], f"Puter æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _openrouter_search(query, cfg):
    """é€é OpenRouter å–å¾—å¤–éƒ¨è³‡è¨Šæ‘˜è¦ï¼ˆå¯ä½¿ç”¨ä½ çš„ OpenRouter Keyï¼‰ã€‚"""
    search_cfg = cfg.get("search", {})
    api_key = _normalize_secret(search_cfg.get("openrouter_api_key"))
    model = search_cfg.get("openrouter_model", "perplexity/sonar")
    if not api_key:
        return [], "OpenRouter æœªè¨­å®š openrouter_api_keyã€‚"

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": search_cfg.get("openrouter_site_url", "https://localhost"),
        "X-Title": search_cfg.get("openrouter_app_name", "stock-project"),
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ç ”ç©¶åŠ©ç†ï¼Œè«‹æ ¹æ“šå…¬é–‹ç¶²è·¯è³‡è¨Šæ•´ç†é‡é»ï¼Œé™„ä¸Šä¾†æºç¶²å€ï¼Œç¦æ­¢è™›æ§‹ã€‚",
            },
            {
                "role": "user",
                "content": f"è«‹é‡å°ä»¥ä¸‹ä¸»é¡Œæ•´ç† 5 é»é‡é»ï¼Œæ¯é»éœ€é™„å¯é»æ“Šç¶²å€ï¼š{query}",
            },
        ],
        "temperature": 0.1,
    }

    try:
        resp = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=30,
        )
        data = resp.json()
        if resp.status_code >= 400:
            err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
            return [], f"OpenRouter æœå°‹å¤±æ•—ï¼š{err_msg}"

        content = data.get("choices", [{}])[0].get("message", {}).get("content", "") if isinstance(data, dict) else ""
        if not content:
            return [], "OpenRouter å·²é€£ç·šï¼Œä½†ç„¡å›å‚³å…§å®¹ã€‚"
        return [{"source": "OpenRouter", "title": "æ‘˜è¦", "snippet": content, "url": "", "tier": "B"}], None
    except Exception as exc:
        return [], f"OpenRouter æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _openrouter_connectivity_check(cfg):
    """å¿«é€Ÿæª¢æŸ¥ OpenRouter æ˜¯å¦å¯ç”±ç›®å‰ç’°å¢ƒæˆåŠŸå‘¼å«ã€‚"""
    records, warning = _openrouter_search("å°è‚¡ ä»Šæ—¥é‡é»æ–°è", cfg)
    if warning:
        return False, warning
    return bool(records), "OpenRouter é€£ç·šæª¢æŸ¥æˆåŠŸã€‚"

def _ddg_search(query, max_results=5, source="DuckDuckGo"):
    try:
        with DDGS() as ddgs:
            results = []
            for r in ddgs.text(query, max_results=max_results):
                results.append(
                    {
                        "source": source,
                        "title": r.get("title", ""),
                        "snippet": r.get("body", ""),
                        "url": r.get("href", ""),
                        "tier": _classify_source_tier(r.get("href", "")),
                    }
                )
            if not results:
                return [], f"{source} æŸ¥è©¢ç„¡çµæœã€‚"
            return results, None
    except Exception as exc:
        return [], f"{source} æœå°‹ä¾‹å¤–ï¼š{str(exc)}"


def _classify_source_tier(url):
    """å°‡ä¾†æºåˆ†ç´šï¼šA(å®˜æ–¹/ç›£ç®¡)ã€B(ä¸»æµåª’é«”)ã€C(å…¶ä»–)ã€‚"""
    url_text = (url or "").lower()
    if not url_text:
        return "C"

    for tier, patterns in TRUSTED_SOURCE_PATTERNS.items():
        if any(p in url_text for p in patterns):
            return tier
    return "C"


def _resolve_us_mapping(stock_id, stock_name):
    """Layer B èµ·é»ï¼šå…ˆåšé«˜å¯ä¿¡ç™½åå–®å°æ‡‰ï¼Œå†ä¿ç•™å¾ŒçºŒæ“´å……ç©ºé–“ã€‚"""
    sid = str(stock_id).strip()
    ticker = TW_US_ADR_MAPPING.get(sid)
    if ticker:
        return {
            "ticker": ticker,
            "mapping_type": "direct_adr",
            "confidence": 0.98,
            "evidence": ["manual_mapping_table"],
        }

    return {
        "ticker": "",
        "mapping_type": "none",
        "confidence": 0.0,
        "evidence": [f"no_mapping_for_{stock_name}_{sid}"],
    }


def _build_search_queries(stock_name, sid):
    """å»ºç«‹å¤šè§’åº¦æŸ¥è©¢ï¼Œè®“å…è²»è¯ç¶²æ‘˜è¦æ›´æ¥è¿‘å¯æœå°‹ LLM çš„æ•ˆæœã€‚"""
    base = f"{stock_name} {sid}"
    return [
        (f"{base} å…¬å¸ç°¡ä»‹ æ ¸å¿ƒç”¢å“ ç”¢æ¥­åœ°ä½", "å…¬å¸å®šä½"),
        (f"{base} æœ€æ–°æ–°è è¨‚å–® å®¢æˆ¶", "æœ€æ–°å‹•æ…‹"),
        (f"{base} æ³•èªªæœƒ è²¡æ¸¬ è³‡æœ¬æ”¯å‡º æ¯›åˆ©ç‡", "ç¶“ç‡Ÿå±•æœ›"),
        (f"{base} é¢¨éšª åŒ¯ç‡ åŸç‰©æ–™ åœ°ç·£æ”¿æ²»", "é¢¨éšªäº‹ä»¶"),
    ]


def _dedup_records(records):
    seen = set()
    deduped = []
    for rec in records:
        key = (rec.get("source", ""), rec.get("title", ""), rec.get("url", ""))
        if key in seen:
            continue
        seen.add(key)
        deduped.append(rec)
    return deduped


def _resolve_rag_config(cfg):
    """è®€å– RAG è¨­å®šï¼›å„ªå…ˆä½¿ç”¨ llm.ragï¼Œä¸¦ç›¸å®¹èˆŠéŒ¯ç½®åˆ° search.rag çš„æƒ…æ³ã€‚"""
    llm_cfg = cfg.get("llm", {}) if isinstance(cfg, dict) else {}
    search_cfg = cfg.get("search", {}) if isinstance(cfg, dict) else {}

    llm_rag = llm_cfg.get("rag", {}) if isinstance(llm_cfg, dict) else {}
    search_rag = search_cfg.get("rag", {}) if isinstance(search_cfg, dict) else {}
    rag_cfg = llm_rag if llm_rag else search_rag

    llm_api_key = _normalize_secret(llm_cfg.get("api_key")) if isinstance(llm_cfg, dict) else ""
    search_api_key = _normalize_secret(search_cfg.get("api_key")) if isinstance(search_cfg, dict) else ""

    return {
        "enabled": str(rag_cfg.get("enabled", "false")).lower() == "true" if isinstance(rag_cfg, dict) else False,
        "embedding_model": (rag_cfg.get("embedding_model") if isinstance(rag_cfg, dict) else None) or "nvidia/nv-embed-v1",
        "top_k": int((rag_cfg.get("top_k") if isinstance(rag_cfg, dict) else 8) or 8),
        "api_key": llm_api_key or search_api_key,
    }


def _embed_texts_nim(api_key, model, texts):
    """å‘¼å« NVIDIA Embeddings API å–å¾—å‘é‡ã€‚"""
    if not api_key or not texts:
        return []

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {"model": model, "input": texts}
    resp = requests.post(
        "https://integrate.api.nvidia.com/v1/embeddings",
        headers=headers,
        json=payload,
        timeout=30,
    )

    data = resp.json() if resp.content else {}
    if resp.status_code >= 400:
        err_msg = data.get("error", {}).get("message", f"HTTP {resp.status_code}") if isinstance(data, dict) else f"HTTP {resp.status_code}"
        raise RuntimeError(f"Embedding API å¤±æ•—ï¼š{err_msg}")

    vectors = []
    for row in data.get("data", []) if isinstance(data, dict) else []:
        vec = row.get("embedding") if isinstance(row, dict) else None
        if isinstance(vec, list):
            vectors.append(vec)
    return vectors


def _cosine_similarity(vec_a, vec_b):
    if not vec_a or not vec_b or len(vec_a) != len(vec_b):
        return -1.0
    dot = sum(a * b for a, b in zip(vec_a, vec_b))
    norm_a = math.sqrt(sum(a * a for a in vec_a))
    norm_b = math.sqrt(sum(b * b for b in vec_b))
    if norm_a == 0 or norm_b == 0:
        return -1.0
    return dot / (norm_a * norm_b)


def _apply_rag_rerank(stock_name, sid, records, cfg):
    """å°å¤–éƒ¨æœé›†çµæœåš embedding ç›¸ä¼¼åº¦æ’åºï¼Œå– top-kã€‚"""
    rag_cfg = _resolve_rag_config(cfg)
    if not rag_cfg["enabled"]:
        return records, None
    if not records:
        return records, "RAG å·²å•Ÿç”¨ï¼Œä½†ç›®å‰æ²’æœ‰å¯é‡æ’åºçš„å¤–éƒ¨è³‡æ–™ã€‚"
    if not rag_cfg["api_key"]:
        return records, "RAG å·²å•Ÿç”¨ä½†ç¼ºå°‘ API keyï¼ˆè«‹è¨­å®š llm.api_key æˆ– search.api_keyï¼‰ã€‚"

    top_k = max(1, min(rag_cfg["top_k"], len(records)))
    query = f"{stock_name} {sid} åŸºæœ¬é¢ å…¬å¸å®šä½ æ–°è é¢¨éšª"
    doc_texts = [f"{r.get('title', '')}\n{r.get('snippet', '')}\n{r.get('url', '')}" for r in records]

    try:
        q_vecs = _embed_texts_nim(rag_cfg["api_key"], rag_cfg["embedding_model"], [query])
        d_vecs = _embed_texts_nim(rag_cfg["api_key"], rag_cfg["embedding_model"], doc_texts)
        if not q_vecs or len(d_vecs) != len(records):
            return records, "RAG é‡æ’åºç•¥éï¼šembedding å›å‚³ä¸å®Œæ•´ã€‚"

        q_vec = q_vecs[0]
        scored = []
        for rec, d_vec in zip(records, d_vecs):
            sim = _cosine_similarity(q_vec, d_vec)
            scored.append((sim, rec))
        scored.sort(key=lambda x: x[0], reverse=True)
        reranked = [rec for _, rec in scored[:top_k]]
        return reranked, f"RAG é‡æ’åºå·²å•Ÿç”¨ï¼šæ¨¡å‹ {rag_cfg['embedding_model']}ï¼Œä¿ç•™ Top-{top_k}ã€‚"
    except Exception as exc:
        return records, f"RAG é‡æ’åºå¤±æ•—ï¼Œå›é€€åŸå§‹æœå°‹çµæœï¼š{str(exc)}"


def _build_external_context(stock_name, sid, cfg):
    """è’é›†å¤–éƒ¨è³‡è¨Šï¼ˆå¯é…ç½®ä»˜è²»/å…è²»ä¾†æº + ç¤¾ç¾¤ç¶²ç«™æœå°‹ï¼‰ã€‚"""
    search_cfg = cfg.get("search", {})
    preferred_provider = str(search_cfg.get("provider", "openrouter_then_rss")).lower().strip()

    records = []
    warnings = []
    topic_queries = _build_search_queries(stock_name, sid)

    openrouter_queries_used = 0
    openrouter_query_budget = int(search_cfg.get("openrouter_queries_per_run", 2) or 2)
    use_ddg = str(search_cfg.get("enable_ddg", "false")).lower() == "true"

    for query, tag in topic_queries:
        if preferred_provider in {"openrouter", "openrouter_then_rss", "openrouter_then_ddg"} and openrouter_queries_used < openrouter_query_budget:
            cache_key = f"or::{query}"
            cached = _external_cache_get(cache_key)
            if cached is not None:
                records.extend(cached)
            else:
                or_records, or_warn = _openrouter_search(query, cfg)
                records.extend(or_records)
                if or_records:
                    _external_cache_set(cache_key, or_records)
                if or_warn:
                    warnings.append(f"[{tag}] {or_warn}")
            openrouter_queries_used += 1
        elif preferred_provider == "perplexity":
            pplx_records, pplx_warn = _perplexity_search(query, cfg)
            records.extend(pplx_records)
            if pplx_warn:
                warnings.append(f"[{tag}] {pplx_warn}")
        elif preferred_provider == "puter":
            put_records, put_warn = _puter_search(query, cfg)
            records.extend(put_records)
            if put_warn:
                warnings.append(f"[{tag}] {put_warn}")

        rss_records, rss_warn = _google_news_rss_search(query, max_results=2)
        records.extend(rss_records)
        if rss_warn:
            warnings.append(f"[{tag}] {rss_warn}")

        if use_ddg or preferred_provider == "openrouter_then_ddg":
            ddg_records, ddg_warn = _ddg_search(query, max_results=2, source=f"DuckDuckGo/{tag}")
            records.extend(ddg_records)
            if ddg_warn:
                warnings.append(f"[{tag}] {ddg_warn}")

    wiki_records, wiki_warn = _wikipedia_summary_search(stock_name, sid)
    records.extend(wiki_records)
    if wiki_warn:
        warnings.append(wiki_warn)

    # ç¤¾ç¾¤/è¼¿æƒ…ï¼ˆå¯é¸ï¼Œé¿å… DDG å“è³ªå·®æ™‚å¼•å…¥å™ªéŸ³ï¼‰
    if use_ddg:
        social_queries = [
            (f"site:x.com OR site:twitter.com {stock_name} {sid}", "X/Twitter"),
            (f"site:facebook.com {stock_name} {sid}", "Facebook"),
            (f"site:instagram.com {stock_name} {sid}", "Instagram"),
        ]
        for query, source in social_queries:
            social_records, social_warn = _ddg_search(query, max_results=2, source=source)
            records.extend(social_records)
            if social_warn:
                warnings.append(social_warn)

    records = _dedup_records(records)
    records, rag_warning = _apply_rag_rerank(stock_name, sid, records, cfg)
    if rag_warning:
        warnings.insert(0, rag_warning)

    if not records:
        warnings.insert(0, "ç›®å‰æœªå–å¾—å¤–éƒ¨ä¾†æºã€‚è«‹å…ˆæª¢æŸ¥ä¸‹æ–¹å„ä¾†æºè¨ºæ–·è¨Šæ¯ã€‚")
        return "", warnings

    source_counts = {}
    tier_counts = {"A": 0, "B": 0, "C": 0}
    for rec in records:
        src = rec.get("source", "ä¾†æº")
        source_counts[src] = source_counts.get(src, 0) + 1
        tier = rec.get("tier", "C")
        tier_counts[tier] = tier_counts.get(tier, 0) + 1
    summary = "ã€".join([f"{k}:{v}" for k, v in source_counts.items()])
    tier_summary = "ã€".join([f"{k}:{v}" for k, v in tier_counts.items() if v > 0])
    warnings.insert(0, f"å¤–éƒ¨ä¾†æºæŠ“å–æˆåŠŸï¼ˆ{summary}ï¼›ä¾†æºåˆ†ç´š {tier_summary}ï¼‰ã€‚å·²åœç”¨ Google Custom Search JSON APIï¼Œæ”¹æ¡ OpenRouter + Google News RSS/Wikipediaã€‚")

    mapping_info = _resolve_us_mapping(sid, stock_name)
    if mapping_info["mapping_type"] == "direct_adr":
        warnings.insert(1, f"ç¾è‚¡å°æ‡‰ï¼š{sid} â†’ {mapping_info['ticker']}ï¼ˆdirect_adr, confidence={mapping_info['confidence']:.2f}ï¼‰")
    else:
        warnings.insert(1, f"ç¾è‚¡å°æ‡‰ï¼šç›®å‰ç„¡ç™½åå–® ADR å°æ‡‰ï¼ˆ{sid}ï¼‰ï¼Œå¾ŒçºŒå¯ç”±å¤–éƒ¨çµæ§‹åŒ–ä¾†æºè£œå¼·ã€‚")

    lines = []
    for rec in records[:24]:
        url = rec.get("url", "")
        url_text = f"ï¼ˆ{url}ï¼‰" if url else ""
        lines.append(
            f"- [{rec.get('source', 'ä¾†æº')}/Tier-{rec.get('tier', 'C')}] "
            f"{rec.get('title', '')}: {rec.get('snippet', '')} {url_text}"
        )
    return "\n".join(lines), warnings


def _fmt_metric(value, fallback="æœªæä¾›"):
    if value is None or value == "":
        return fallback
    return str(value)


def _to_float(value):
    if value is None:
        return None
    text = str(value).replace(",", "").replace("%", "").strip()
    if text in {"", "æœªæä¾›", "N/A", "nan"}:
        return None
    try:
        return float(text)
    except (TypeError, ValueError):
        return None


def _free_score_label(score):
    if score >= 2:
        return "åå¤š"
    if score <= -2:
        return "åä¿å®ˆ"
    return "ä¸­æ€§"


def _latest_metric_value(financial_df, metric_names):
    """å¾è²¡å ±æ˜ç´°ä¸­å–å‡ºæŒ‡å®šæŒ‡æ¨™çš„æœ€æ–°å€¼ã€‚"""
    if financial_df.empty:
        return None

    norm_names = {str(n).strip().lower() for n in metric_names}
    matched = financial_df[financial_df["type"].astype(str).str.strip().str.lower().isin(norm_names)]
    if matched.empty:
        return None
    return matched.iloc[0]["value"]


def _fmt_percent(value):
    val = _to_float(value)
    if val is None:
        return "æœªæä¾›"
    return f"{val:.2f}%"


def _compute_data_quality(metrics):
    required = [
        "latest_eps",
        "prev_eps",
        "latest_revenue",
        "oldest_revenue",
        "revenue_growth",
        "roe",
        "roa",
        "gross_margin",
        "operating_cf",
    ]
    available = sum(1 for key in required if _to_float(metrics.get(key)) is not None)
    ratio = available / len(required)

    if ratio >= 0.8:
        return "é«˜", ratio
    if ratio >= 0.5:
        return "ä¸­", ratio
    return "ä½", ratio


def _build_free_fundamental_report(stock_name, sid, search_ctx, metrics):
    latest_eps = _to_float(metrics.get("latest_eps"))
    prev_eps = _to_float(metrics.get("prev_eps"))
    revenue_growth = _to_float(metrics.get("revenue_growth"))

    eps_trend = "è³‡æ–™ä¸è¶³"
    if latest_eps is not None and prev_eps is not None:
        eps_trend = "æˆé•·" if latest_eps > prev_eps else ("ä¸‹æ»‘" if latest_eps < prev_eps else "æŒå¹³")

    score = 0
    if latest_eps is not None:
        score += 1 if latest_eps > 0 else -1
    if revenue_growth is not None:
        score += 1 if revenue_growth > 0 else -1
    if latest_eps is not None and prev_eps is not None and latest_eps > prev_eps:
        score += 1

    risk_note = "å¸‚å ´æ™¯æ°£å¾ªç’°èˆ‡ç”¢æ¥­ç«¶çˆ­å¯èƒ½å½±éŸ¿ç‡Ÿæ”¶èˆ‡ç²åˆ©ã€‚"
    if latest_eps is not None and latest_eps < 0:
        risk_note = "ç›®å‰ EPS ç‚ºè² ï¼Œéœ€å„ªå…ˆé—œæ³¨è™§ææ”¶æ–‚èˆ‡ç¾é‡‘æµå“è³ªã€‚"
    elif revenue_growth is not None and revenue_growth < 0:
        risk_note = "è¿‘æœŸç‡Ÿæ”¶æˆé•·ç‡ç‚ºè² ï¼Œéœ€ç•™æ„éœ€æ±‚æ”¾ç·©æˆ–ç”¢å“çµ„åˆè®ŠåŒ–ã€‚"

    ext_note = "æœªå–å¾—å¤–éƒ¨æ–°èæ‘˜è¦ã€‚"
    if search_ctx:
        ext_note = "å·²ç´å…¥ OpenRouter / RSS / Wikipedia ç­‰å¤–éƒ¨æ‘˜è¦ï¼Œä¸¦äº¤å‰å°ç…§è³‡æ–™åº«æ•¸æ“šã€‚"

    data_quality_level, data_quality_ratio = _compute_data_quality(metrics)

    return f"""
## å…¬å¸ç°¡ä»‹
{stock_name}ï¼ˆ{sid}ï¼‰ç‚ºå°è‚¡ä¸Šå¸‚æ«ƒå…¬å¸ï¼Œæœ¬å ±å‘Šæ¡ç”¨å…§éƒ¨è³‡æ–™åº«è²¡å ±æ¬„ä½èˆ‡å…è²»å¤–éƒ¨æœå°‹æ‘˜è¦é€²è¡Œæ•´ç†ã€‚

## è²¡å‹™åˆ†æ
ç›®å‰è§€å¯Ÿåˆ° EPS è¶¨å‹¢ç‚ºã€Œ{eps_trend}ã€ï¼Œæ•´é«”è²¡å‹™å‹•èƒ½åˆ¤è®€ç‚ºã€Œ{_free_score_label(score)}ã€ã€‚
{ext_note}
åˆ†æé‡é»ä»¥ã€Œç²åˆ©é€£çºŒæ€§ï¼ˆEPSï¼‰ï¼‹æˆé•·æ–¹å‘ï¼ˆç‡Ÿæ”¶ï¼‰ï¼‹è³‡ç”¢å“è³ªï¼ˆROE/ROA/ç¾é‡‘æµï¼‰ã€ä¸‰è»¸äº¤å‰åˆ¤è®€ã€‚

### è²¡å‹™æŒ‡æ¨™
- EPSï¼š{_fmt_metric(metrics.get('latest_eps'))}
- ROEï¼ˆè‚¡æ±æ¬Šç›Šå ±é…¬ç‡ï¼‰ï¼š{_fmt_percent(metrics.get('roe'))}
- ROAï¼ˆè³‡ç”¢å ±é…¬ç‡ï¼‰ï¼š{_fmt_percent(metrics.get('roa'))}
- ç‡Ÿæ”¶æˆé•·ç‡ï¼š{_fmt_metric(metrics.get('revenue_growth'))}
- æ¯›åˆ©ç‡ï¼š{_fmt_percent(metrics.get('gross_margin'))}

## ç‡Ÿæ”¶åˆ†æ
è¿‘ 12 æœˆç‡Ÿæ”¶ç”± { _fmt_metric(metrics.get('oldest_revenue')) } å„„è®ŠåŒ–è‡³ { _fmt_metric(metrics.get('latest_revenue')) } å„„ï¼Œæˆé•·ç‡ç‚º { _fmt_metric(metrics.get('revenue_growth')) }ã€‚
è‹¥æˆé•·ç‡è½‰å¼±ï¼Œé€šå¸¸ä»£è¡¨çµ‚ç«¯éœ€æ±‚ã€ç”¢å“åƒ¹æ ¼æˆ–å‡ºè²¨ç¯€å¥æ‰¿å£“ã€‚

## æ¯›åˆ©ç‡åˆ†æ
ç›®å‰è³‡æ–™åº«æœªæä¾›å¯ç›´æ¥è¨ˆç®—çš„æœ€æ–°æ¯›åˆ©ç‡æ¬„ä½ï¼Œå»ºè­°å¾ŒçºŒè£œé½Šå­£å ±æ¯›åˆ©ç‡ä»¥æå‡åˆ¤è®€ç²¾åº¦ã€‚

## ç¾é‡‘æµé‡åˆ†æ
ç‡Ÿæ¥­ç¾é‡‘æµï¼ˆOperating Cash Flowï¼‰ï¼š{_fmt_metric(metrics.get('operating_cf'))}ã€‚
è‹¥ç‡Ÿæ”¶èˆ‡ç²åˆ©æˆé•·ä½†ç¾é‡‘æµæœªåŒæ­¥æ”¹å–„ï¼Œéœ€ç•™æ„æ‡‰æ”¶å¸³æ¬¾ã€åº«å­˜èˆ‡è³‡æœ¬æ”¯å‡ºå£“åŠ›ã€‚

## æŠ•è³‡è©•åƒ¹
- çŸ­æœŸè©•åƒ¹ï¼š{_free_score_label(score)}ï¼ˆä»¥ç‡Ÿæ”¶èˆ‡ EPS æœ€æ–°è®ŠåŒ–ç‚ºä¸»ï¼‰
- ä¸­æœŸè©•åƒ¹ï¼šä¸­æ€§ååŸºæœ¬é¢é©—è­‰ï¼ˆéœ€è¿½è¹¤é€£çºŒ 2~3 å­£ EPS èˆ‡ç‡Ÿæ”¶æ˜¯å¦åŒå‘ï¼‰
- é•·æœŸè©•åƒ¹ï¼šå–æ±ºæ–¼ç”¢å“ç«¶çˆ­åŠ›ã€è³‡æœ¬æ”¯å‡ºæ•ˆç‡ã€è‡ªç”±ç¾é‡‘æµèˆ‡æ™¯æ°£å¾ªç’°ä½ç½®
- ç›®æ¨™åƒ¹æ ¼ï¼šè³‡æ–™ä¸è¶³ï¼ˆå…è²»ç‰ˆä¸ç”¢ç”Ÿç›®æ¨™åƒ¹ï¼‰

## é¢¨éšªè©•ä¼°
- å¸‚å ´é¢¨éšªï¼šå—ç¸½é«”æ™¯æ°£ã€åˆ©ç‡èˆ‡è³‡é‡‘é¢å½±éŸ¿
- è²¡å‹™é¢¨éšªï¼š{risk_note}
- æ³•è¦/æ”¿ç­–é¢¨éšªï¼šéœ€ç•™æ„ç”¢æ¥­æ”¿ç­–ã€å‡ºå£ç®¡åˆ¶èˆ‡æœƒè¨ˆæº–å‰‡è®Šå‹•

## çµè«–
æœ¬æ¬¡ç‚ºã€Œå¼·åŒ–ç‰ˆå…è²» AI åŸºæœ¬é¢åˆ†æã€ï¼Œä»¥å¯é©—è­‰æ•¸æ“šåšè¦å‰‡åŒ–æ‘˜è¦ï¼›è‹¥å•Ÿç”¨ LLM å¯å†é€²ä¸€æ­¥åšè„ˆçµ¡æ•´åˆã€‚
ç›®å‰è³‡æ–™å®Œæ•´åº¦è©•ä¼°ï¼š{data_quality_level}ï¼ˆ{data_quality_ratio:.0%}ï¼‰ã€‚
å»ºè­°å¾ŒçºŒæŒçºŒè¿½è¹¤ï¼šEPS é€£çºŒæ€§ã€ç‡Ÿæ”¶å¹´å¢ç‡è½‰æŠ˜ã€ç¾é‡‘æµå“è³ªï¼Œä»¥åŠé‡å¤§æ–°èäº‹ä»¶å°è¨‚å–®èˆ‡æ¯›åˆ©ç‡çš„å½±éŸ¿ã€‚
""".strip()


def _build_fundamental_prompt(stock_name, sid, search_ctx, metrics):
    """å»ºç«‹å›ºå®šç« ç¯€æ ¼å¼çš„åŸºæœ¬é¢åˆ†æ Promptã€‚"""
    return f"""
è«‹ä½ æ‰®æ¼”å°è‚¡è³‡æ·±åŸºæœ¬é¢åˆ†æå¸«ï¼Œé‡å° {stock_name}ï¼ˆ{sid}ï¼‰æ’°å¯«å ±å‘Šã€‚

ã€é‡è¦è¦å‰‡ã€‘
1) åš´æ ¼ä½¿ç”¨ä»¥ä¸‹å›ºå®šæ ¼å¼èˆ‡æ¨™é¡Œé †åºï¼Œä¸è¦å¢æ¸›ç« ç¯€ã€‚
2) è‹¥è³‡æ–™ä¸è¶³ï¼Œè«‹æ˜ç¢ºå¯«ã€Œæœªæä¾›ã€æˆ–ã€Œè³‡æ–™ä¸è¶³ã€ï¼Œç¦æ­¢è™›æ§‹ã€‚
3) æ‰€æœ‰æ•¸å€¼ç›¡é‡å¼•ç”¨æˆ‘æä¾›çš„è³‡æ–™ï¼›è‹¥å¼•ç”¨æ–°èï¼Œåƒ…èƒ½ä½¿ç”¨ã€Œæœå°‹äº‹å¯¦æ‘˜è¦ã€ã€‚
4) è‹¥æœ‰å¤–éƒ¨äº‹ä»¶ï¼Œè«‹åœ¨å¥å°¾åŠ ä¸Šå°æ‡‰ä¾†æºç¶²å€ï¼ˆå¯å¤šç­†ï¼‰ã€‚
5) ä»¥ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚

ã€å›ºå®šè¼¸å‡ºæ ¼å¼ã€‘
## å…¬å¸ç°¡ä»‹
ï¼ˆå…¬å¸å®šä½ã€æ ¸å¿ƒç”¢å“/æœå‹™ã€ä¸»è¦å¸‚å ´ï¼‰

## è²¡å‹™åˆ†æ
ï¼ˆæ•´é«”ç²åˆ©èƒ½åŠ›èˆ‡è¿‘æ³ï¼Œ2-4 å¥ï¼‰

### è²¡å‹™æŒ‡æ¨™
- EPSï¼š
- ROEï¼ˆè‚¡æ±æ¬Šç›Šå ±é…¬ç‡ï¼‰ï¼š
- ROAï¼ˆè³‡ç”¢å ±é…¬ç‡ï¼‰ï¼š
- ç‡Ÿæ”¶æˆé•·ç‡ï¼š
- æ¯›åˆ©ç‡ï¼š

## ç‡Ÿæ”¶åˆ†æ
ï¼ˆç‡Ÿæ”¶è¶¨å‹¢ã€å¯èƒ½é©…å‹•å› å­ï¼‰

## æ¯›åˆ©ç‡åˆ†æ
ï¼ˆæ¯›åˆ©ç‡æ°´æº–èˆ‡å¯èƒ½åŸå› ï¼›è‹¥ç„¡è³‡æ–™è«‹å¯«æœªæä¾›ï¼‰

## ç¾é‡‘æµé‡åˆ†æ
ï¼ˆç¾é‡‘æµé‡ç‹€æ³èˆ‡å“è³ªï¼›è‹¥ç„¡è³‡æ–™è«‹å¯«æœªæä¾›ï¼‰

## æŠ•è³‡è©•åƒ¹
- çŸ­æœŸè©•åƒ¹ï¼š
- ä¸­æœŸè©•åƒ¹ï¼š
- é•·æœŸè©•åƒ¹ï¼š
- ç›®æ¨™åƒ¹æ ¼ï¼š

## é¢¨éšªè©•ä¼°
- å¸‚å ´é¢¨éšªï¼š
- è²¡å‹™é¢¨éšªï¼š
- æ³•è¦/æ”¿ç­–é¢¨éšªï¼š

## çµè«–
ï¼ˆç¸½çµæŠ•è³‡è§€é»èˆ‡é—œéµè¿½è¹¤æŒ‡æ¨™ï¼‰

ã€å¯ç”¨è³‡æ–™ã€‘
- æœå°‹äº‹å¯¦æ‘˜è¦ï¼š{search_ctx if search_ctx else 'æœªæä¾›'}
- æœ€æ–°å­£åº¦ EPSï¼š{metrics.get('latest_eps', 'æœªæä¾›')}
- ä¸Šå­£ EPSï¼š{metrics.get('prev_eps', 'æœªæä¾›')}
- è¿‘ 12 æœˆæœ€æ–°ç‡Ÿæ”¶ï¼ˆå„„å…ƒï¼‰ï¼š{metrics.get('latest_revenue', 'æœªæä¾›')}
- è¿‘ 12 æœˆæœ€èˆŠç‡Ÿæ”¶ï¼ˆå„„å…ƒï¼‰ï¼š{metrics.get('oldest_revenue', 'æœªæä¾›')}
- ä¼°ç®—ç‡Ÿæ”¶æˆé•·ç‡ï¼ˆæœ€æ–° vs æœ€èˆŠï¼‰ï¼š{metrics.get('revenue_growth', 'æœªæä¾›')}
- ROEï¼š{metrics.get('roe', 'æœªæä¾›')}
- ROAï¼š{metrics.get('roa', 'æœªæä¾›')}
- æ¯›åˆ©ç‡ï¼š{metrics.get('gross_margin', 'æœªæä¾›')}
- ç‡Ÿæ¥­ç¾é‡‘æµï¼š{metrics.get('operating_cf', 'æœªæä¾›')}
""".strip()

# 1. æ ¸å¿ƒ AI å‘¼å«å·¥å…· (ä¿æŒç©©å®šï¼Œæœªæ›´å‹•)
def _call_nim_fundamental(cfg, prompt):
    llm_cfg = cfg.get("llm", {})
    api_key = _normalize_secret(llm_cfg.get("api_key"))
    model_name = get_llm_model(cfg, "fundamental", "meta/llama-3.1-70b-instruct")
    if not api_key:
        raise ValueError("llm.api_key æœªè¨­å®šã€‚")

    url = "https://integrate.api.nvidia.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è­‰åˆ¸åˆ†æå¸«ã€‚è«‹å„ªå…ˆåƒè€ƒé€£ç¶²æœå°‹åˆ°çš„äº‹å¯¦ï¼Œçµåˆè²¡å‹™æ•¸æ“šçµ¦å‡ºå…·é«”çš„æŠ•è³‡è©•åƒ¹ï¼Œåš´ç¦è™›æ§‹å…¬å¸æ¥­å‹™ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.1
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    try:
        data = resp.json()
    except Exception:
        data = {}

    if resp.status_code >= 400:
        err_msg = data.get("error", {}).get("message") if isinstance(data, dict) else None
        raise RuntimeError(err_msg or f"NIM API å‘¼å«å¤±æ•—ï¼ˆHTTP {resp.status_code}ï¼‰ã€‚")

    content = data.get("choices", [{}])[0].get("message", {}).get("content", "") if isinstance(data, dict) else ""
    if not content:
        raise RuntimeError("NIM API æœªå›å‚³å¯ç”¨å…§å®¹ã€‚")
    return content

def show_fundamental_analysis():
    st.markdown("### ğŸ’ åŸºæœ¬é¢æ•¸æ“šå…¨è¦½èˆ‡ AI è¨ºæ–·")
    cfg = database.load_config()
    conn = database.get_db_connection(cfg)
    universe = cfg.get("universe", [])
    stock_options = {f"{s['stock_id']} {s['name']}": s['stock_id'] for s in universe}
    
    selected_stock = st.selectbox("è«‹é¸æ“‡åˆ†ææ¨™çš„", list(stock_options.keys()))
    sid = stock_options[selected_stock]
    
    # çµ±ä¸€åŸºæº–æ—¥
    def_s = pd.to_datetime("today") - pd.Timedelta(days=60)
    def_e = pd.to_datetime("today")
    f_range = st.date_input("æ•¸æ“šè§€å¯ŸåŸºæº–æ—¥", value=[def_s, def_e])
    end_d = f_range[1] if len(f_range) == 2 else def_e

    st.divider()

    # --- 1. æ•¸æ“šæŠ“å–èˆ‡æ ¼å¼åŒ– ---
    
    # (1) æ¯æœˆç‡Ÿæ”¶ï¼šé™¤ä»¥ 1000 ä¸¦åŠ ä¸Šåƒåˆ†ä½ ","
    rev_query = f"SELECT date as 'æ—¥æœŸ', revenue FROM stock_month_revenue_monthly WHERE stock_id='{sid}' ORDER BY date DESC LIMIT 12"
    rev_df = pd.read_sql(rev_query, conn)
    if not rev_df.empty:
        # âœ… ä¿®æ­£é—œéµï¼šçµ±ä¸€æ¬„ä½åç¨±ç‚º 'ç‡Ÿæ”¶(å„„)'ï¼Œä¸¦åŠ ä¸Šåƒåˆ†ä½
        rev_df['ç‡Ÿæ”¶(å„„)'] = (rev_df['revenue'] / 100000000).apply(lambda x: f"{x:,.2f}")
        rev_df = rev_df[['æ—¥æœŸ', 'ç‡Ÿæ”¶(å„„)']]
    
    # (2+4) æ¯å­£ç²åˆ©èˆ‡ EPSï¼šå®‰å…¨è½‰ç½®è™•ç†
    metric_candidates = [
        "EPS", "Net Profit", "ROE", "ROE(%)", "Return on Equity",
        "ROA", "ROA(%)", "Return on Assets",
        "Gross Margin", "Gross Margin(%)", "æ¯›åˆ©ç‡",
        "Operating Cash Flow", "ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰", "ç‡Ÿæ¥­ç¾é‡‘æµ",
    ]
    metric_filter = ", ".join([f"'{m}'" for m in metric_candidates])
    profit_raw = pd.read_sql(
        f"SELECT date, type, value FROM stock_financial_statements WHERE stock_id='{sid}' AND type IN ({metric_filter}) ORDER BY date DESC LIMIT 200",
        conn,
    )
    if not profit_raw.empty:
        profit_df = profit_raw.pivot(index='date', columns='type', values='value').reset_index()
        # å®‰å…¨é‡å‘½å
        rename_map = {'date': 'å­£åº¦', 'EPS': 'EPS', 'Net Profit': 'æ¯å­£ç²åˆ©'}
        profit_df = profit_df.rename(columns={k: v for k, v in rename_map.items() if k in profit_df.columns})
        # åŠ ä¸Šåƒåˆ†ä½ (æ¯å­£ç²åˆ©)
        if 'æ¯å­£ç²åˆ©' in profit_df.columns:
            profit_df['æ¯å­£ç²åˆ©'] = profit_df['æ¯å­£ç²åˆ©'].apply(lambda x: f"{x:,.0f}" if pd.notnull(x) else "N/A")
    else:
        profit_df = pd.DataFrame()

    # (3+5) æœ¬ç›Šæ¯”èˆ‡æ®–åˆ©ç‡
    val_query = f"SELECT date as 'æ—¥æœŸ', PER as 'æœ¬ç›Šæ¯”', dividend_yield as 'æ®–åˆ©ç‡%' FROM stock_per_pbr_daily WHERE stock_id='{sid}' AND date <= '{end_d}' ORDER BY date DESC LIMIT 10"
    valuation_df = pd.read_sql(val_query, conn)

    # (6+7) ç¾é‡‘è‚¡åˆ©èˆ‡è‚¡æ¯åˆ†é…
    div_query = f"SELECT year as 'å¹´ä»½', CashEarningsDistribution as 'ç¾é‡‘è‚¡åˆ©', StockEarningsDistribution as 'è‚¡ç¥¨è‚¡åˆ©' FROM stock_dividend WHERE stock_id='{sid}' ORDER BY year DESC LIMIT 5"
    div_df = pd.read_sql(div_query, conn)

    # --- 2. è¡¨æ ¼åŒ–é¡¯ç¤ºåˆ†é  ---
    tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ ç‡Ÿæ”¶èˆ‡ç²åˆ©è©³æƒ…", "ğŸ’° è‚¡åˆ©èˆ‡ä¼°å€¼çœ‹æ¿", "ğŸ” AI è¯ç¶²è¶¨å‹¢å ±å‘Š"])

    with tab1:
        st.write("#### 1. æ¯æœˆç‡Ÿæ”¶ (å–®ä½ï¼šç™¾è¬å…ƒ)")
        st.dataframe(rev_df, use_container_width=True, hide_index=True)
        
        st.write("#### 2 & 4. æ¯å­£ç²åˆ©èˆ‡ EPS æ­·ç¨‹")
        if not profit_df.empty:
            # ç¢ºä¿åƒ…é¡¯ç¤ºç¾æœ‰æ¬„ä½
            cols_to_show = [c for c in ['å­£åº¦', 'æ¯å­£ç²åˆ©', 'EPS'] if c in profit_df.columns]
            st.dataframe(profit_df[cols_to_show], use_container_width=True, hide_index=True)
        else:
            st.info("å°šç„¡ç²åˆ©æ•¸æ“šã€‚")

    with tab2:
        st.write("#### 3 & 5. æœ¬ç›Šæ¯”èˆ‡æ®–åˆ©ç‡è®Šå‹•")
        st.dataframe(valuation_df, use_container_width=True, hide_index=True)
        
        st.write("#### 6 & 7. æ­·å¹´è‚¡åˆ©åˆ†é… (ç¾é‡‘èˆ‡è‚¡ç¥¨)")
        if not div_df.empty:
            st.table(div_df)
        else:
            st.info("å°šç„¡è‚¡åˆ©æ­·å²æ•¸æ“šã€‚")

    with tab3:
        llm_cfg = cfg.get("llm", {})
        llm_available = bool(_normalize_secret(llm_cfg.get("api_key")))

        use_llm = st.toggle(
            "å•Ÿç”¨ LLM å¼·åŒ–åˆ†æï¼ˆå¯é¸ï¼‰",
            value=llm_available,
            help="è‹¥å·²è¨­å®š llm.api_keyï¼Œå»ºè­°é–‹å•Ÿï¼›æœªå•Ÿç”¨æ™‚ç³»çµ±å°‡ä½¿ç”¨å…è²»è¦å‰‡åŒ–æ‘˜è¦ã€‚",
        )
        model_name = st.text_input(
            "LLM æ¨¡å‹ï¼ˆNVIDIA NIMï¼‰",
            value=get_llm_model(cfg, "fundamental", "meta/llama-3.1-70b-instruct"),
            disabled=not use_llm,
        )

        if use_llm and not llm_available:
            st.warning("ç›®å‰æœªè¨­å®š llm.api_keyï¼Œå°‡è‡ªå‹•å›é€€åˆ°å…è²»è¦å‰‡åŒ–å ±å‘Šã€‚")

        if llm_available:
            st.success(f"âœ… å·²åµæ¸¬åˆ° llm.api_keyï¼ˆ{_mask_secret(llm_cfg.get('api_key'))}ï¼‰ï¼Œå¯ç›´æ¥ä½¿ç”¨ {model_name} é€²è¡Œå¼·åŒ–åˆ†æã€‚")
        st.info("ğŸ’¡ æ”¹å–„å»ºè­°ï¼šç³»çµ±æœƒå…ˆåšå¤šæŸ¥è©¢è¯ç¶²è’é›†ï¼Œå†äº¤çµ¦ LLM æ•´åˆï¼›æ•ˆæœæœƒæ¯”åªé æ¨¡å‹è¨˜æ†¶å¥½ã€‚")
        st.caption("æ­¤é æ”¯æ´ç´” NVIDIA LLM åˆ†æï¼›è‹¥æœªè¨­å®š llm.api_keyï¼Œç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨å…è²»è¦å‰‡åŒ–å ±å‘Šã€‚")

        run_btn_label = "ğŸš€ å•Ÿå‹• AI åŸºæœ¬é¢åˆ†æï¼ˆLLM å¼·åŒ–ï¼‰" if use_llm else "ğŸš€ å•Ÿå‹• AI åŸºæœ¬é¢åˆ†æï¼ˆå…è²»è¦å‰‡åŒ–ï¼‰"
        # âœ… ä¿ç•™è¯ç¶²æœå°‹é‚è¼¯
        if st.button(run_btn_label, use_container_width=True):
            with st.spinner("æ­£åœ¨æœå°‹æœ€æ–°ç”¢æ¥­åœ°ä½èˆ‡å¸‚å ´æ–°è..."):
                search_ctx, search_warnings = _build_external_context(selected_stock, sid, cfg)
                if search_warnings:
                    for w in search_warnings:
                        if w.startswith("å¤–éƒ¨ä¾†æºæŠ“å–æˆåŠŸ"):
                            st.success(w)
                        else:
                            st.warning(w)
                
                # ç²å– AI åƒè€ƒæ•¸æ“š
                latest_eps = profit_df['EPS'].iloc[0] if ('EPS' in profit_df.columns and not profit_df.empty) else "æœªæä¾›"
                prev_eps = profit_df['EPS'].iloc[1] if ('EPS' in profit_df.columns and len(profit_df) > 1) else "æœªæä¾›"

                latest_revenue = rev_df['ç‡Ÿæ”¶(å„„)'].iloc[0] if not rev_df.empty else "æœªæä¾›"
                oldest_revenue = rev_df['ç‡Ÿæ”¶(å„„)'].iloc[-1] if not rev_df.empty else "æœªæä¾›"

                revenue_growth = "æœªæä¾›"
                if not rev_df.empty and len(rev_df) > 1:
                    rev_num = pd.to_numeric(rev_df['ç‡Ÿæ”¶(å„„)'].str.replace(',', ''), errors='coerce')
                    latest_rev_num = rev_num.iloc[0]
                    oldest_rev_num = rev_num.iloc[-1]
                    if pd.notnull(latest_rev_num) and pd.notnull(oldest_rev_num) and oldest_rev_num != 0:
                        revenue_growth = f"{((latest_rev_num - oldest_rev_num) / oldest_rev_num) * 100:.2f}%"

                metrics = {
                    "latest_eps": latest_eps,
                    "prev_eps": prev_eps,
                    "latest_revenue": latest_revenue,
                    "oldest_revenue": oldest_revenue,
                    "revenue_growth": revenue_growth,
                    "roe": _latest_metric_value(profit_raw, ["ROE", "Return on Equity", "ROE(%)"]),
                    "roa": _latest_metric_value(profit_raw, ["ROA", "Return on Assets", "ROA(%)"]),
                    "gross_margin": _latest_metric_value(profit_raw, ["Gross Margin", "Gross Margin(%)", "æ¯›åˆ©ç‡"]),
                    "operating_cf": _latest_metric_value(profit_raw, ["Operating Cash Flow", "ç‡Ÿæ¥­æ´»å‹•ä¹‹æ·¨ç¾é‡‘æµå…¥ï¼ˆæµå‡ºï¼‰", "ç‡Ÿæ¥­ç¾é‡‘æµ"]),
                }

                if use_llm and llm_available:
                    cfg.setdefault("llm", {}).setdefault("models", {})["fundamental"] = model_name
                    prompt = _build_fundamental_prompt(selected_stock, sid, search_ctx, metrics)
                    try:
                        ai_report = _call_nim_fundamental(cfg, prompt)
                        st.markdown(ai_report)
                    except Exception as exc:
                        st.error(f"LLM å‘¼å«å¤±æ•—ï¼Œæ”¹ç”¨å…è²»è¦å‰‡åŒ–å ±å‘Šï¼š{str(exc)}")
                        st.markdown(_build_free_fundamental_report(selected_stock, sid, search_ctx, metrics))
                else:
                    st.markdown(_build_free_fundamental_report(selected_stock, sid, search_ctx, metrics))

    conn.close()
